<!DOCTYPE html>
<html lang="en">
<head>
    <title>
        ThRaSH Seminars
    </title>
    <!-- Next line is for the nice mobile view -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="thrash.css">
</head>
<body>

    <nav class="nav-container">
        <div class="nav-menu">
            <ul class="menu-list">
                <li class="menu-item"><a href="https://thrash-seminars.github.io/pre/">Previous</a></li>
                <li class="menu-item"><a href="https://thrash-seminars.github.io/">Home</a></li>
            </ul>
        </div>
    </nav>

    <div class="main">
        <h1>ThRaSH Seminars Series, Autumn 2023 &ndash; Winter 2024</h1>

        <p><b>When?</b> On Tuesdays at 14:00 CE(S)T. This is UTC 12:00 before 29<sup>th</sup> October and UTC 13:00 after that.</p>

        <p><b>Where?</b> Here is a <a href="https://cnrs.zoom.us/j/95228771980?pwd=N1NDQ245TmV4ZDdKaC9aL05hc0l5UT09">permanent zoom link</a>.</p>

        <p><b>Mailing list?</b> Subscribe to the seminar mailing list <a href="https://www.jiscmail.ac.uk/cgi-bin/wa-jisc.exe?A0=THRASH-SEMINARS" target="_blank">here</a>. If you want to get all the ThRaSH-related news, subscribe to <a href="https://www.jiscmail.ac.uk/cgi-bin/wa-jisc.exe?A0=THRASH" target="_blank">this list</a> as well.</p>

        <p><b>Organizers:</b> <a href="https://www.lix.polytechnique.fr/Labo/Martin.KREJCA/">Martin Krejca</a> and <a href="https://cs.adelaide.edu.au/~denis/">Denis Antipov</a>.</p>

        <h2 id="Schedule">Schedule</h2>

        <p>All seminars will be added to <a href="https://calendar.google.com/calendar/embed?src=00ae99cd98c6cb6e82b2594b37d22900dd8f8c441a587bb45bb87be550a6c8c7%40group.calendar.google.com" target="_blank">this google calendar</a> (the times are displayed in CE(S)T). To subscribe to this calendar, <a href="https://calendar.google.com/calendar/u/0?cid=MDBhZTk5Y2Q5OGM2Y2I2ZTgyYjI1OTRiMzdkMjI5MDBkZDhmOGM0NDFhNTg3YmI0NWJiODdiZTU1MGE2YzhjN0Bncm91cC5jYWxlbmRhci5nb29nbGUuY29t" target="_blank">use this link</a>.</p>

        <ul>
            <li><a href="#Oct24"><b>24 Oct:</b> Timo K&ouml;tzing, <i>The Conditioning of Drift Theorems: To Filtrate or Not To Filtrate?</i></a></li>
            <li><a href="#Oct31"><b>31 Oct:</b> Carsten Witt, <i>First Steps Towards a Runtime Analysis of Neuroevolution</i></a></li>
            <li><a href="#Nov7"><b>7 Nov:</b> Dirk Sudholt, <i>Runtime Analysis of Quality Diversity Algorithms</i></a></li>
        </ul>

        <h2>Abstracts of the talks</h2>

        <dl id="Oct24">
            <dt><strong>24 October 2023</strong></dt>
                
            <dd>
                <p><i>The Conditioning of Drift Theorems: To Filtrate or Not To Filtrate?</i> &mdash; <a href="https://hpi.de/friedrich/people/timo-koetzing.html">Timo K&ouml;tzing</a>, Hasso Plattner Institute, Potsdam, Germany</p>

                <p>The talk is dedicated to the conditions of the expectations in the drift analysis. While often we need to condition on filtrations, in the analysis of evolutionary algorithms we can use more specific conditions, e.g., condition on the previous states of the process. The goal of this talk is to underline the differences of these approaches and to discuss, which drift theorems conditions would be most useful in the real world.</p>
            </dd>
        </dl>

        <dl id="Oct31">
            <dt><strong>31 October 2023</strong></dt>
                
            <dd>
                <p><i>First Steps Towards a Runtime Analysis of Neuroevolution</i> &mdash; <a href="https://www.imm.dtu.dk/~cawi/">Carsten Witt</a>, Technical University of Denmark, Lyngby, Denmark</p>

                <p>We consider a simple setting in neuroevolution where an evolutionary algorithm optimizes the weights and activation functions of a simple artificial neural network. We then define simple example functions to be learned by the network and conduct rigorous runtime analyses for networks with a single neuron and for a more advanced structure with several neurons and two layers. Our results show that the proposed algorithm is generally efficient on two example problems designed for one neuron and efficient with at least constant probability on the example problem for a two-layer network. In particular, the so-called harmonic mutation operator choosing steps of size j with probability proportional to 1/j turns out as a good choice for the underlying search space. However, for the case of one neuron, we also identify situations with hard-to-overcome local optima.</p>
                
                <p>This is joint work with Paul Fischer and Emil Lundt Larsen</p>
            </dd>
        </dl>

        <dl id="Nov7">
            <dt><strong>7 November 2023</strong></dt>
                
            <dd>
                <p><i>Runtime Analysis of Quality Diversity Algorithms</i> &mdash; <a href="https://www.fim.uni-passau.de/en/intelligent-systems/team/chair-holder">Dirk Sudholt</a>, the University of Passau, Passau, Germany</p>

                <p>Quality diversity (QD) is a branch of evolutionary computation that gained increasing interest in recent years. The Map-Elites QD approach defines a feature space, i.e., a partition of the search space, and stores the best solution for each cell of this space. We study a simple QD algorithm in the context of pseudo-Boolean optimisation on the “number of ones” feature space, where the ith cell stores the best solution amongst those with a number of ones in [(i − 1)k, ik − 1]. Here k is a granularity parameter 1 ≤ k ≤ n + 1. We give a tight bound on the expected time until all cells are covered for arbitrary fitness functions and for all k and analyse the expected optimisation time of QD on OneMax and other problems whose structure aligns favourably with the feature space. On combinatorial problems we show that QD finds a (1 − 1/e)-approximation when maximising any monotone sub-modular function with a single uniform cardinality constraint efficiently. Defining the feature space as the number of connected components of a connected graph, we show that QD finds a minimum spanning tree in expected polynomial time.</p>
                
                <p>We further consider QD’s performance on classes of transformed functions in which the feature space is not well aligned with the problem. The asymptotic performance is unaffected by transformations on easy functions like OneMax. Applying a worst-case transformation to a deceptive problem increases the expected optimisation time from O(n2 log n) to an exponential time. However, QD is still faster than a (1+1) EA by an exponential factor.</p>

                <p>Joint work with Jakob Bossek (a GECCO 2023 paper with a subset of journal extensions)</p>
            </dd>
        </dl>
    </div>
</body>